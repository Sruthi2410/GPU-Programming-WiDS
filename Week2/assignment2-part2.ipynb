{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T20:13:55.162078Z","iopub.execute_input":"2025-12-21T20:13:55.162324Z","iopub.status.idle":"2025-12-21T20:13:55.329076Z","shell.execute_reply.started":"2025-12-21T20:13:55.162298Z","shell.execute_reply":"2025-12-21T20:13:55.328196Z"}},"outputs":[{"name":"stdout","text":"Sun Dec 21 20:13:55 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   39C    P0             25W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile vector_add.cu\n#include <iostream>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void vectorAdd(const float* a, const float* b, float* c, int n) {\n    int idx = blockIdx.x;   // one thread per block\n\n    if (idx < n) {\n        c[idx] = a[idx] + b[idx];\n    }\n}\n\nint main() {\n    int n = 16;\n    size_t size = n * sizeof(float);\n\n    float *h_a = new float[n];\n    float *h_b = new float[n];\n    float *h_c = new float[n];\n\n    for (int i = 0; i < n; i++) {\n        h_a[i] = i * 1.0f;\n        h_b[i] = 2.0f * i;\n        h_c[i] = 0.0f;\n    }\n\n    float *d_a, *d_b, *d_c;\n    cudaMalloc(&d_a, size);\n    cudaMalloc(&d_b, size);\n    cudaMalloc(&d_c, size);\n\n    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n\n    // n blocks, 1 thread per block\n    vectorAdd<<<n, 1>>>(d_a, d_b, d_c, n);\n    cudaDeviceSynchronize();\n\n    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n\n    std::cout << \"Results:\\n\";\n    for (int i = 0; i < n; i++) {\n        std::cout << \"i=\" << i\n                  << \" a=\" << h_a[i]\n                  << \" b=\" << h_b[i]\n                  << \" c=\" << h_c[i]\n                  << std::endl;\n    }\n\n    bool correct = true;\n    for (int i = 0; i < n; i++) {\n        float expected = h_a[i] + h_b[i];\n        if (fabs(h_c[i] - expected) > 1e-5) {\n            correct = false;\n            break;\n        }\n    }\n\n    std::cout << \"\\nVector Add: \"\n              << (correct ? \"PASS\" : \"FAIL\") << std::endl;\n\n    delete[] h_a;\n    delete[] h_b;\n    delete[] h_c;\n    cudaFree(d_a);\n    cudaFree(d_b);\n    cudaFree(d_c);\n\n    return 0;\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T20:18:21.800188Z","iopub.execute_input":"2025-12-21T20:18:21.800979Z","iopub.status.idle":"2025-12-21T20:18:21.807307Z","shell.execute_reply.started":"2025-12-21T20:18:21.800935Z","shell.execute_reply":"2025-12-21T20:18:21.806692Z"}},"outputs":[{"name":"stdout","text":"Writing vector_add.cu\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T20:18:43.921990Z","iopub.execute_input":"2025-12-21T20:18:43.922574Z","iopub.status.idle":"2025-12-21T20:18:44.040169Z","shell.execute_reply.started":"2025-12-21T20:18:43.922550Z","shell.execute_reply":"2025-12-21T20:18:44.039477Z"}},"outputs":[{"name":"stdout","text":"vector_add.cu\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!sed -n '1,20p' vector_add.cu\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T20:19:18.598146Z","iopub.execute_input":"2025-12-21T20:19:18.598816Z","iopub.status.idle":"2025-12-21T20:19:18.716127Z","shell.execute_reply.started":"2025-12-21T20:19:18.598787Z","shell.execute_reply":"2025-12-21T20:19:18.715274Z"}},"outputs":[{"name":"stdout","text":"#include <iostream>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void vectorAdd(const float* a, const float* b, float* c, int n) {\n    int idx = blockIdx.x;   // one thread per block\n\n    if (idx < n) {\n        c[idx] = a[idx] + b[idx];\n    }\n}\n\nint main() {\n    int n = 16;\n    size_t size = n * sizeof(float);\n\n    float *h_a = new float[n];\n    float *h_b = new float[n];\n    float *h_c = new float[n];\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!nvcc vector_add.cu -o vector_add\n!./vector_add\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T20:19:38.388496Z","iopub.execute_input":"2025-12-21T20:19:38.388788Z","iopub.status.idle":"2025-12-21T20:19:41.515613Z","shell.execute_reply.started":"2025-12-21T20:19:38.388762Z","shell.execute_reply":"2025-12-21T20:19:41.514938Z"}},"outputs":[{"name":"stdout","text":"Results:\ni=0 a=0 b=0 c=0\ni=1 a=1 b=2 c=3\ni=2 a=2 b=4 c=6\ni=3 a=3 b=6 c=9\ni=4 a=4 b=8 c=12\ni=5 a=5 b=10 c=15\ni=6 a=6 b=12 c=18\ni=7 a=7 b=14 c=21\ni=8 a=8 b=16 c=24\ni=9 a=9 b=18 c=27\ni=10 a=10 b=20 c=30\ni=11 a=11 b=22 c=33\ni=12 a=12 b=24 c=36\ni=13 a=13 b=26 c=39\ni=14 a=14 b=28 c=42\ni=15 a=15 b=30 c=45\n\nVector Add: PASS\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile vector_add.cu\n#include <iostream>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void vectorAdd(const float* a, const float* b, float* c, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        c[idx] = a[idx] + b[idx];\n    }\n}\n\nint main() {\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    float *h_a = new float[n];\n    float *h_b = new float[n];\n    float *h_c = new float[n];\n\n    for (int i = 0; i < n; i++) {\n        h_a[i] = i * 1.0f;\n        h_b[i] = 2.0f * i;\n    }\n\n    float *d_a, *d_b, *d_c;\n    cudaMalloc(&d_a, size);\n    cudaMalloc(&d_b, size);\n    cudaMalloc(&d_c, size);\n\n    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n\n    int blockSize = 256;\n    int gridSize = (n + blockSize - 1) / blockSize;\n\n    vectorAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, n);\n    cudaDeviceSynchronize();\n\n    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n\n    bool correct = true;\n    for (int i = 0; i < n; i++) {\n        float expected = h_a[i] + h_b[i];\n        if (fabs(h_c[i] - expected) > 1e-5) {\n            correct = false;\n            break;\n        }\n    }\n\n    std::cout << \"Vector Add: \" << (correct ? \"PASS\" : \"FAIL\") << std::endl;\n\n    delete[] h_a;\n    delete[] h_b;\n    delete[] h_c;\n    cudaFree(d_a);\n    cudaFree(d_b);\n    cudaFree(d_c);\n\n    return 0;\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T20:28:36.162550Z","iopub.execute_input":"2025-12-21T20:28:36.163271Z","iopub.status.idle":"2025-12-21T20:28:36.169409Z","shell.execute_reply.started":"2025-12-21T20:28:36.163239Z","shell.execute_reply":"2025-12-21T20:28:36.168742Z"}},"outputs":[{"name":"stdout","text":"Overwriting vector_add.cu\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%%writefile multiply_scale.cu\n#include <iostream>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void multiplyScale(const float* a, const float* b, float* c,\n                              float alpha, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        c[idx] = alpha * a[idx] * b[idx];\n    }\n}\n\nint main() {\n    int n = 1000000;\n    float alpha = 0.5f;\n    size_t size = n * sizeof(float);\n\n    float *h_a = new float[n];\n    float *h_b = new float[n];\n    float *h_c = new float[n];\n\n    for (int i = 0; i < n; i++) {\n        h_a[i] = i * 1.0f;\n        h_b[i] = 3.0f;\n    }\n\n    float *d_a, *d_b, *d_c;\n    cudaMalloc(&d_a, size);\n    cudaMalloc(&d_b, size);\n    cudaMalloc(&d_c, size);\n\n    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n\n    int blockSize = 256;\n    int gridSize = (n + blockSize - 1) / blockSize;\n\n    multiplyScale<<<gridSize, blockSize>>>(d_a, d_b, d_c, alpha, n);\n    cudaDeviceSynchronize();\n\n    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n\n    bool correct = true;\n    for (int i = 0; i < n; i++) {\n        float expected = alpha * h_a[i] * h_b[i];\n        if (fabs(h_c[i] - expected) > 1e-5) {\n            correct = false;\n            break;\n        }\n    }\n\n    std::cout << \"Multiply Scale: \" << (correct ? \"PASS\" : \"FAIL\") << std::endl;\n\n    delete[] h_a;\n    delete[] h_b;\n    delete[] h_c;\n    cudaFree(d_a);\n    cudaFree(d_b);\n    cudaFree(d_c);\n\n    return 0;\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T20:28:57.343345Z","iopub.execute_input":"2025-12-21T20:28:57.343618Z","iopub.status.idle":"2025-12-21T20:28:57.348905Z","shell.execute_reply.started":"2025-12-21T20:28:57.343596Z","shell.execute_reply":"2025-12-21T20:28:57.348163Z"}},"outputs":[{"name":"stdout","text":"Writing multiply_scale.cu\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"%%writefile relu.cu\n#include <iostream>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void relu(const float* x, float* y, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        y[idx] = (x[idx] > 0.0f) ? x[idx] : 0.0f;\n    }\n}\n\nint main() {\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    float *h_x = new float[n];\n    float *h_y = new float[n];\n\n    for (int i = 0; i < n; i++) {\n        h_x[i] = (i % 2 == 0) ? i * 1.0f : -i * 1.0f;\n    }\n\n    float *d_x, *d_y;\n    cudaMalloc(&d_x, size);\n    cudaMalloc(&d_y, size);\n\n    cudaMemcpy(d_x, h_x, size, cudaMemcpyHostToDevice);\n\n    int blockSize = 256;\n    int gridSize = (n + blockSize - 1) / blockSize;\n\n    relu<<<gridSize, blockSize>>>(d_x, d_y, n);\n    cudaDeviceSynchronize();\n\n    cudaMemcpy(h_y, d_y, size, cudaMemcpyDeviceToHost);\n\n    bool correct = true;\n    for (int i = 0; i < n; i++) {\n        float expected = (h_x[i] > 0.0f) ? h_x[i] : 0.0f;\n        if (fabs(h_y[i] - expected) > 1e-5) {\n            correct = false;\n            break;\n        }\n    }\n\n    std::cout << \"ReLU: \" << (correct ? \"PASS\" : \"FAIL\") << std::endl;\n\n    delete[] h_x;\n    delete[] h_y;\n    cudaFree(d_x);\n    cudaFree(d_y);\n\n    return 0;\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T20:29:18.954184Z","iopub.execute_input":"2025-12-21T20:29:18.954474Z","iopub.status.idle":"2025-12-21T20:29:18.959958Z","shell.execute_reply.started":"2025-12-21T20:29:18.954452Z","shell.execute_reply":"2025-12-21T20:29:18.959245Z"}},"outputs":[{"name":"stdout","text":"Writing relu.cu\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!nvcc vector_add.cu -o vector_add\n!./vector_add\n\n!nvcc multiply_scale.cu -o multiply_scale\n!./multiply_scale\n\n!nvcc relu.cu -o relu\n!./relu\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T20:29:40.719563Z","iopub.execute_input":"2025-12-21T20:29:40.720211Z","iopub.status.idle":"2025-12-21T20:29:46.865831Z","shell.execute_reply.started":"2025-12-21T20:29:40.720184Z","shell.execute_reply":"2025-12-21T20:29:46.864938Z"}},"outputs":[{"name":"stdout","text":"Vector Add: PASS\nMultiply Scale: PASS\nReLU: PASS\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"%%writefile vector_add_2.cu\n#include <iostream>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void vectorAdd(const float* a, const float* b, float* c, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        c[idx] = a[idx] + b[idx];\n    }\n}\n\nint main() {\n    int Ns[] = {1000, 100000, 10000000};\n    int blockSizes[] = {32, 128, 256, 512};\n\n    for (int ni = 0; ni < 3; ni++) {\n        int n = Ns[ni];\n        size_t size = n * sizeof(float);\n\n        float *h_a = new float[n];\n        float *h_b = new float[n];\n        float *h_c = new float[n];\n\n        for (int i = 0; i < n; i++) {\n            h_a[i] = i * 1.0f;\n            h_b[i] = 2.0f * i;\n        }\n\n        float *d_a, *d_b, *d_c;\n        cudaMalloc(&d_a, size);\n        cudaMalloc(&d_b, size);\n        cudaMalloc(&d_c, size);\n\n        cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n        cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n\n        std::cout << \"\\nInput size n = \" << n << std::endl;\n\n        for (int bi = 0; bi < 4; bi++) {\n            int blockSize = blockSizes[bi];\n            int gridSize = (n + blockSize - 1) / blockSize;\n\n            vectorAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, n);\n            cudaDeviceSynchronize();\n\n            cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n\n            bool correct = true;\n            for (int i = 0; i < n; i++) {\n                float expected = h_a[i] + h_b[i];\n                if (fabs(h_c[i] - expected) > 1e-5) {\n                    correct = false;\n                    break;\n                }\n            }\n\n            std::cout << \"  blockSize = \" << blockSize\n                      << \", gridSize = \" << gridSize\n                      << \", totalThreads = \" << gridSize * blockSize\n                      << \" -> \" << (correct ? \"PASS\" : \"FAIL\")\n                      << std::endl;\n        }\n\n        delete[] h_a;\n        delete[] h_b;\n        delete[] h_c;\n        cudaFree(d_a);\n        cudaFree(d_b);\n        cudaFree(d_c);\n    }\n\n    return 0;\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T20:43:26.704968Z","iopub.execute_input":"2025-12-21T20:43:26.705660Z","iopub.status.idle":"2025-12-21T20:43:26.712339Z","shell.execute_reply.started":"2025-12-21T20:43:26.705628Z","shell.execute_reply":"2025-12-21T20:43:26.711620Z"}},"outputs":[{"name":"stdout","text":"Writing vector_add_2.cu\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!nvcc vector_add_2.cu -o vector_add_2\n!./vector_add_2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T20:44:06.887008Z","iopub.execute_input":"2025-12-21T20:44:06.887762Z","iopub.status.idle":"2025-12-21T20:44:09.269606Z","shell.execute_reply.started":"2025-12-21T20:44:06.887734Z","shell.execute_reply":"2025-12-21T20:44:09.268934Z"}},"outputs":[{"name":"stdout","text":"\nInput size n = 1000\n  blockSize = 32, gridSize = 32, totalThreads = 1024 -> PASS\n  blockSize = 128, gridSize = 8, totalThreads = 1024 -> PASS\n  blockSize = 256, gridSize = 4, totalThreads = 1024 -> PASS\n  blockSize = 512, gridSize = 2, totalThreads = 1024 -> PASS\n\nInput size n = 100000\n  blockSize = 32, gridSize = 3125, totalThreads = 100000 -> PASS\n  blockSize = 128, gridSize = 782, totalThreads = 100096 -> PASS\n  blockSize = 256, gridSize = 391, totalThreads = 100096 -> PASS\n  blockSize = 512, gridSize = 196, totalThreads = 100352 -> PASS\n\nInput size n = 10000000\n  blockSize = 32, gridSize = 312500, totalThreads = 10000000 -> PASS\n  blockSize = 128, gridSize = 78125, totalThreads = 10000000 -> PASS\n  blockSize = 256, gridSize = 39063, totalThreads = 10000128 -> PASS\n  blockSize = 512, gridSize = 19532, totalThreads = 10000384 -> PASS\n","output_type":"stream"}],"execution_count":12}]}